{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "     ---------------------------------------- 9.2/9.2 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.1-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "     ---------------------------------------- 44.0/44.0 MB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in e:\\project sentiment analysis\\projenv\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def readCorpus(fpath):\n",
    "    sents = []\n",
    "    with open(fpath) as fd:\n",
    "        sent = []\n",
    "        for l in fd:\n",
    "            #lt = l.strip().decode(\"utf8\")\n",
    "            lt = l.strip()\n",
    "            if not lt:\n",
    "                sents.append(sent)\n",
    "                sent = []\n",
    "            else:\n",
    "                w_t = lt.split('\\t')\n",
    "                sent.append([w_t[0], w_t[1]])\n",
    "    return sents\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        #'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        #'word[-3:]=' + word[-3:],\n",
    "        #'word[-2:]=' + word[-2:],\n",
    "        #'word.isupper=%s' % word.isupper(),\n",
    "        #'word.istitle=%s' % word.istitle(),\n",
    "        #'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        #'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        pass\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            #'-1:word.istitle=%s' % word1.istitle(),\n",
    "            #'-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            #'-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        pass\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            #'+1:word.istitle=%s' % word1.istitle(),\n",
    "            #'+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            #'+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start append train set.\n",
      "append train set done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x1ca2c492850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents = readCorpus(\"E:/Project Sentiment analysis/CRF/test.txt\")\n",
    "x_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "\n",
    "print(\"start append train set.\")\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "print(\"append train set done.\")\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,\n",
    "    'c2': 1e-3,\n",
    "    'max_iterations': 500,\n",
    "    'feature.possible_transitions': True\n",
    "    })\n",
    "\n",
    "trainer.train(\"trained_model\")\n",
    "test_object = pycrfsuite.Tagger()\n",
    "test_object.open(\"trained_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
